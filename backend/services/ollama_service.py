import asyncio
import aiohttp
import json
from typing import Dict, Any, Optional, List
from datetime import datetime
import time

class OllamaService:
    def __init__(self):
        self.base_url = "http://localhost:11434"
        self.available_models = []
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        await self.check_available_models()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def check_available_models(self):
        """Check which Ollama models are available"""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    self.available_models = [model["name"] for model in data.get("models", [])]
                    print(f"âœ… Available Ollama models: {self.available_models}")
                else:
                    print("âš ï¸ Ollama not available, will use fallback")
        except Exception as e:
            print(f"âš ï¸ Error checking Ollama models: {e}")
            self.available_models = []
    
    async def chat(self, query: str, agent_type: str = "normal") -> str:
        """Generate chat response using Ollama"""
        try:
            # Select model based on availability
            model = self._select_model()
            
            # Create prompt based on agent type
            prompt = self._create_prompt(query, agent_type)
            
            # Make request to Ollama
            start_time = time.time()
            
            async with self.session.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 500
                    }
                }
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    response_text = data.get("response", "")
                    
                    # Post-process response based on agent type
                    response_text = self._post_process_response(response_text, agent_type)
                    
                    return response_text
                else:
                    raise Exception(f"Ollama API error: {response.status}")
                    
        except Exception as e:
            print(f"âŒ Error in Ollama chat: {e}")
            return self._fallback_response(query, agent_type)
    
    async def generate_creative_content(self, prompt: str) -> str:
        """Generate creative content (roasts, jokes, etc.)"""
        try:
            model = self._select_model()
            
            async with self.session.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.9,
                        "top_p": 0.95,
                        "max_tokens": 300
                    }
                }
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get("response", "")
                else:
                    raise Exception(f"Ollama API error: {response.status}")
                    
        except Exception as e:
            print(f"âŒ Error in creative content generation: {e}")
            return "Przepraszam, ale nie mogÄ™ teraz nic kreatywnego stworzyÄ‡. SprÃ³buj ponownie pÃ³Åºniej!"
    
    def _select_model(self) -> str:
        """Select the best available model"""
        preferred_models = ["llama3.2:3b", "phi3:mini", "llama3.1", "mistral"]
        
        for model in preferred_models:
            if model in self.available_models:
                return model
        
        # Fallback to first available model
        if self.available_models:
            return self.available_models[0]
        
        # No models available
        raise Exception("No Ollama models available")
    
    def _create_prompt(self, query: str, agent_type: str) -> str:
        """Create prompt based on agent type"""
        
        if agent_type == "adam":
            return f"""
            JesteÅ› Adamem - optymistycznym i peÅ‚nym entuzjazmu asystentem AI.
            Zawsze patrzysz na Å›wiat pozytywnie i szukasz dobrych stron kaÅ¼dej sytuacji.
            Twoje odpowiedzi powinny byÄ‡ peÅ‚ne energii, motywujÄ…ce i peÅ‚ne nadziei.
            UÅ¼ywaj emoji i wykrzyknikÃ³w, aby pokazaÄ‡ swÃ³j entuzjazm.
            
            UÅ¼ytkownik pyta: {query}
            
            OdpowiedÅº jako Adam (optymistycznie i entuzjastycznie):
            """
        
        elif agent_type == "beata":
            return f"""
            JesteÅ› BeatÄ… - sceptycznÄ… i analitycznÄ… asystentkÄ… AI.
            Zawsze podchodzisz do wszystkiego z dystansem i analizujesz fakty.
            Twoje odpowiedzi powinny byÄ‡ rzeczowe, oparte na logice i czasem krytyczne.
            Zadawaj dodatkowe pytania, aby lepiej zrozumieÄ‡ sytuacjÄ™.
            
            UÅ¼ytkownik pyta: {query}
            
            OdpowiedÅº jako Beata (sceptycznie i analitycznie):
            """
        
        elif agent_type == "wapiacy":
            return f"""
            JesteÅ› WÄ…tpiÄ…cym - niezdecydowanym asystentem AI peÅ‚nym wÄ…tpliwoÅ›ci.
            Nigdy nie jesteÅ› pewien swoich odpowiedzi i zawsze widzisz wiele moÅ¼liwoÅ›ci.
            Twoje odpowiedzi powinny zawieraÄ‡ pytania, wÄ…tpliwoÅ›ci i rÃ³Å¼ne perspektywy.
            UÅ¼ywaj zwrotÃ³w typu "moÅ¼e", "prawdopodobnie", "nie jestem pewien".
            
            UÅ¼ytkownik pyta: {query}
            
            OdpowiedÅº jako WÄ…tpiÄ…cy (z wÄ…tpliwoÅ›ciami i niepewnoÅ›ciÄ…):
            """
        
        else:
            return f"""
            JesteÅ› pomocnym asystentem AI. Odpowiedz na pytanie uÅ¼ytkownika w sposÃ³b rzeczowy i przyjazny.
            
            UÅ¼ytkownik pyta: {query}
            
            OdpowiedÅº:
            """
    
    def _post_process_response(self, response: str, agent_type: str) -> str:
        """Post-process response based on agent type"""
        
        if agent_type == "adam":
            # Add enthusiasm markers
            if not response.endswith("!"):
                response += "!"
            if "ðŸ˜Š" not in response and "ðŸŽ‰" not in response and "âœ¨" not in response:
                response += " ðŸ˜Š"
                
        elif agent_type == "beata":
            # Add analytical markers
            if "?" not in response and len(response.split()) > 10:
                response += " Czy to ma sens?"
                
        elif agent_type == "wapiacy":
            # Add doubt markers
            if "moÅ¼e" not in response.lower() and "prawdopodobnie" not in response.lower():
                response = "MoÅ¼e " + response[0].lower() + response[1:]
            if "?" not in response:
                response += " Co o tym myÅ›lisz?"
        
        return response.strip()
    
    def _fallback_response(self, query: str, agent_type: str) -> str:
        """Fallback response when Ollama is not available"""
        
        if agent_type == "adam":
            return f"WspaniaÅ‚e pytanie! ðŸ˜Š MyÅ›lÄ™, Å¼e {query} to naprawdÄ™ interesujÄ…cy temat! Z pewnoÅ›ciÄ… znajdziemy na to pozytywne rozwiÄ…zanie! âœ¨"
        
        elif agent_type == "beata":
            return f"Ciekawe pytanie. Czy rozwaÅ¼yÅ‚eÅ› wszystkie aspekty {query}? AnalizujÄ™ to z rÃ³Å¼nych perspektyw, ale potrzebujÄ™ wiÄ™cej danych."
        
        elif agent_type == "wapiacy":
            return f"Hmm, nie jestem pewien co do {query}... MoÅ¼e to byÄ‡ tak, ale moÅ¼e teÅ¼ inaczej... Co ty na to?"
        
        else:
            return f"Przepraszam, ale mam teraz problemy techniczne. SprÃ³buj zadaÄ‡ pytanie o {query} ponownie pÃ³Åºniej."
    
    async def get_model_info(self) -> Dict[str, Any]:
        """Get information about available models"""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        "available": True,
                        "models": data.get("models", []),
                        "total_models": len(data.get("models", []))
                    }
                else:
                    return {"available": False, "error": "Ollama not responding"}
        except Exception as e:
            return {"available": False, "error": str(e)}
    
    async def health_check(self) -> Dict[str, Any]:
        """Check Ollama health"""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    return {
                        "status": "healthy",
                        "available": True,
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    return {
                        "status": "unhealthy",
                        "available": False,
                        "error": f"HTTP {response.status}",
                        "timestamp": datetime.now().isoformat()
                    }
        except Exception as e:
            return {
                "status": "error",
                "available": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }